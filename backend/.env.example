# LLM: ollama(로컬 GPU) | gemini | groq | grok
LLM_PROVIDER=groq

# Ollama - 로컬 RTX 3060 등 GPU 사용, API 키 불필요 (https://ollama.com)
# 1) Ollama 설치 후 터미널에서: ollama run llama3.2 (또는 phi3, mistral 등)
# 2) 아래 설정 후 LLM_PROVIDER=ollama 로 변경
# OLLAMA_BASE_URL=http://localhost:11434/v1
# OLLAMA_MODEL=llama3.2

# Gemini (LLM_PROVIDER=gemini 일 때)
# GEMINI_API_KEY=...
# GEMINI_MODEL=gemini-2.0-flash

# Groq 무료 한도 넉넉 - https://console.groq.com/keys
GROQ_API_KEY=your_groq_api_key_here
# GROQ_MODEL=llama-3.3-70b-versatile

# Grok(xAI) 유료 - https://console.x.ai/
# GROK_API_KEY=...
# GROK_MODEL=grok-2

# DB (PostgreSQL)
DATABASE_URL=postgresql+asyncpg://myuser:password@localhost:5432/expense_db

# Redis
REDIS_URL=redis://localhost:6379/0
